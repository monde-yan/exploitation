import DeplacementPresidentFunction.TextToDeplacementPresidentFunc;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import beans.DeplacementPresident;

import java.util.function.Supplier;

@Slf4j
@RequiredArgsConstructor

public class DeplacementPresidentReceiver implements Supplier<JavaDStream<DeplacementPresident>> {

    private final String inputPathStr;
    private final JavaStreamingContext jsc;

    private final TextToDeplacementPresidentFunc textToDeplacementPresidentFunc = new TextToDeplacementPresidentFunc();
    private final Function<String, DeplacementPresident> mapper = textToDeplacementPresidentFunc::apply;



 private final    Function <Path , Boolean> filter = hdfs ->{
        return !hdfs.getName().startsWith(".")&&
               !hdfs.getName().startsWith("_")&&
               !hdfs.getName().endsWith(".csv");
    };


    @Override
    public JavaDStream<DeplacementPresident> get() {

        JavaPairInputDStream<LongWritable,Text> inputDStream = jsc
                .fileStream(
                        inputPathStr,
                        LongWritable.class,
                        Text.class,
                        TextInputFormat.class,
                        filter,
                        true
                );
        inputDStream.print();
        return inputDStream.map(t -> t._2().toString()).map(mapper);
    }
}
