import DeplacementPresidentFunction.Processor.DeplacementPresidentStreamProcessor;
import DeplacementPresidentFunction.Receiver.DeplacementPresidentReceiver;
import beans.DeplacementPresident;
import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;
import lombok.extern.slf4j.Slf4j;
import org.apache.hadoop.fs.FileSystem;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

import java.io.IOException;


@Slf4j
public class Straming2Main {
    public static void main(String[] args) throws IOException, InterruptedException {



        System.out.println("Hello world!");


        Config config = ConfigFactory.load("application.conf") ;
        String masterUrl = config.getString("app.spark.Master") ;
        String appName = config.getString("app.spark.appName") ;

        String inputPath = config.getString("app.data.input") ;
        String outputPath = config.getString("app.data.output") ;
        String checkpointPath = config.getString("app.data.checkpoint");

        log.info("\ninputPath={}\noutputPath={}\ncheckPoint={}", inputPath, outputPath, checkpointPath);
        log.info("\nspark.MasterUrl={}\nspark.appName={}", masterUrl, appName);

        SparkSession sparkSession = SparkSession.builder().master(masterUrl).appName(appName).getOrCreate();
        JavaStreamingContext jsc = JavaStreamingContext.getOrCreate(
                checkpointPath,
                () -> {
                    JavaStreamingContext javaStreamingContext = new JavaStreamingContext(
                            JavaSparkContext.fromSparkContext(sparkSession.sparkContext()),
                            new Duration(1000 * 10)
                    );
                    javaStreamingContext.checkpoint(checkpointPath);
                    return javaStreamingContext;

                },
                sparkSession.sparkContext().hadoopConfiguration()
        );

        FileSystem hdfs = FileSystem.get(sparkSession.sparkContext().hadoopConfiguration());
        log.info("fileSystem got from sparkSession in the main : hdfs.getScheme = {}", hdfs.getScheme());



        jsc.start();
        jsc.awaitTerminationOrTimeout(1000 * 60 * 8);

    }
}